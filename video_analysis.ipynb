{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ видео-интервью: Транскрибация, Суммаризация и Ответы на вопросы\n",
    "\n",
    "Здравствуйте! Этот Jupyter Notebook — ваш инструмент для полного анализа видео-интервью. Мы пройдем по всем шагам, которые обсуждали: от извлечения аудио до получения ответов на конкретные вопросы.\n",
    "\n",
    "**Ваши технические характеристики (CPU, 32GB RAM, NVIDIA RTX 3060 12GB)** отлично подходят для этой задачи. Мы будем использовать вашу видеокарту (GPU) для ускорения вычислений, что значительно сократит время обработки.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 0: Установка необходимых библиотек\n",
    "\n",
    "Прежде чем мы начнем, необходимо установить все инструменты. Выполните ячейку ниже, чтобы установить все зависимости. Это может занять несколько минут.\n",
    "\n",
    "**Важно:** Команда ниже установит `PyTorch` с поддержкой CUDA (для вашей видеокарты NVIDIA), что критически важно для скорости. Убедитесь, что у вас установлены драйверы NVIDIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка всех необходимых библиотек\n",
    "# Мы используем !pip, чтобы выполнить команду установки прямо из Jupyter Notebook\n",
    "\n",
    "# Устанавливаем PyTorch с поддержкой CUDA 12.1. Это позволит использовать вашу RTX 3060 для ускорения.\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Устанавливаем Whisper от OpenAI для транскрибации\n",
    "!pip install -U openai-whisper\n",
    "\n",
    "# Устанавливаем остальные библиотеки\n",
    "!pip install -U pyannote.audio moviepy transformers[sentencepiece] ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важное замечание по `pyannote.audio`\n",
    "\n",
    "Библиотека `pyannote.audio`, которую мы будем использовать для разделения голосов, требует аутентификации через Hugging Face Hub.\n",
    "\n",
    "**Что нужно сделать:**\n",
    "1.  Перейдите на сайт [Hugging Face](https://huggingface.co) и зарегистрируйтесь (это бесплатно).\n",
    "2.  Перейдите в свой профиль -> Settings -> [Access Tokens](https://huggingface.co/settings/tokens).\n",
    "3.  Создайте новый токен доступа (New token). Можете дать ему любое имя, роль оставьте `read`.\n",
    "4.  Скопируйте этот токен. Он понадобится нам в следующей ячейке.\n",
    "\n",
    "Также вам нужно будет перейти на страницы моделей, которые мы будем использовать, и согласиться с их условиями использования (просто перейдите по ссылкам и нажмите на кнопку согласия, если она появится):\n",
    "*   [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)\n",
    "*   [pyannote/segmentation-3.0](https://huggingface.co/pyannote/segmentation-3.0)\n",
    "\n",
    "Просто перейдите по ссылкам, и если там будет кнопка для согласия с условиями, нажмите ее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: Подготовка моделей и обработка видео\n",
    "\n",
    "Теперь, когда все установлено, мы можем начать. Сначала импортируем все необходимые модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем системные библиотеки\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Импортируем библиотеки для работы с аудио/видео\n",
    "import whisper\n",
    "import moviepy.editor as mp\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# Импортируем библиотеки для суммаризации и QA\n",
    "from transformers import pipeline\n",
    "\n",
    "# Утилиты Jupyter\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from getpass import getpass\n",
    "\n",
    "# Проверяем, доступна ли видеокарта NVIDIA, и устанавливаем устройство\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Используемое устройство: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"Видеокарта: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Введите ваш токен Hugging Face\n",
    "\n",
    "Вставьте ваш токен, который вы скопировали с сайта Hugging Face, в поле ниже. Это необходимо для загрузки модели `pyannote`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass('Введите ваш токен Hugging Face и нажмите Enter: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка моделей\n",
    "\n",
    "Теперь загрузим все необходимые модели. Это может занять некоторое время, так как модели будут скачиваться из интернета при первом запуске. Они сохранятся на вашем диске, и в последующие разы загрузка будет мгновенной.\n",
    "\n",
    "*   **Diarization Pipeline (`pyannote/speaker-diarization-3.1`)**: Модель для определения, кто и когда говорит.\n",
    "*   **Whisper (`medium`)**: Модель для преобразования речи в текст. Мы используем версию `medium`, так как она обеспечивает хороший баланс между точностью и скоростью для вашей RTX 3060."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем, что токен введен\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Токен Hugging Face не введен. Пожалуйста, выполните предыдущую ячейку и вставьте токен.\")\n",
    "\n",
    "try:\n",
    "    print(\"Загрузка модели для диаризации (pyannote)...\")\n",
    "    # Инициализируем pipeline для диаризации (распознавания спикеров)\n",
    "    diarization_pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=hf_token\n",
    "    )\n",
    "    # Перемещаем модель на GPU для ускорения\n",
    "    diarization_pipeline.to(torch.device(DEVICE))\n",
    "    print(\"Модель для диаризации успешно загружена.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке модели pyannote: {e}\")\n",
    "    print(\"Пожалуйста, убедитесь, что вы ввели правильный токен и приняли условия использования моделей на Hugging Face (ссылки были в самом начале ноутбука).\")\n",
    "\n",
    "print(\"\\nЗагрузка модели для транскрибации (Whisper)...\")\n",
    "# Загружаем модель Whisper. 'medium' - хороший баланс качества и скорости.\n",
    "whisper_model = whisper.load_model(\"medium\", device=DEVICE)\n",
    "print(\"Модель Whisper успешно загружена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение аудио из видеофайла\n",
    "\n",
    "Теперь укажите путь к вашему видеофайлу. Код извлечет из него аудиодорожку и сохранит в формате `.wav` в той же папке, где лежит этот ноутбук."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем текстовое поле для ввода пути к файлу\n",
    "video_path_input = widgets.Text(\n",
    "    description='Путь к видео:',\n",
    "    placeholder='C:/Users/User/Videos/interview.mp4',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='90%')\n",
    ")\n",
    "display(video_path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта ячейка выполняется после того, как вы вставили путь в поле выше и запустили ее\n",
    "video_path_str = video_path_input.value\n",
    "\n",
    "# Проверяем, был ли введен путь\n",
    "if not video_path_str:\n",
    "    raise ValueError(\"Путь к видеофайлу не указан. Пожалуйста, введите путь в ячейке выше и выполните ее снова.\")\n",
    "\n",
    "# Убираем кавычки, если пользователь случайно их добавил (часто при копировании пути в Windows)\n",
    "video_path_str = video_path_str.strip('\"')\n",
    "video_path = Path(video_path_str)\n",
    "\n",
    "# Проверяем, существует ли файл\n",
    "if not video_path.is_file():\n",
    "    raise FileNotFoundError(f\"Файл не найден по указанному пути: {video_path}\")\n",
    "\n",
    "print(f\"Обрабатывается файл: {video_path.name}\")\n",
    "\n",
    "# Определяем путь для сохранения аудиофайла\n",
    "# Аудио будет сохранено в той же папке, где лежит ноутбук, с именем 'temp_audio.wav'\n",
    "audio_path = Path('temp_audio.wav')\n",
    "\n",
    "print(\"Извлечение аудиодорожки... Это может занять некоторое время для больших файлов.\")\n",
    "\n",
    "# Используем moviepy для извлечения аудио\n",
    "try:\n",
    "    # Загружаем видеоклип\n",
    "    with mp.VideoFileClip(str(video_path)) as video_clip:\n",
    "        audio_clip = video_clip.audio\n",
    "        # Сохраняем аудио в формате WAV, который лучше всего подходит для Whisper\n",
    "        # acodec='pcm_s16le' - стандартный WAV кодек\n",
    "        # ar=16000 - частота дискретизации 16кГц, стандарт для моделей распознавания речи\n",
    "        audio_clip.write_audiofile(audio_path, codec='pcm_s16le', fps=16000)\n",
    "    \n",
    "    print(f\"Аудио успешно извлечено и сохранено в файл: {audio_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при извлечении аудио: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Транскрибация с разделением спикеров\n",
    "\n",
    "Это главный и самый долгий этап. Здесь мы сначала применим модель диаризации, чтобы найти отрезки речи каждого спикера, а затем используем Whisper для транскрибации каждого такого отрезка. \n",
    "\n",
    "**Внимание:** В зависимости от длины вашего видео, выполнение этой ячейки может занять от нескольких минут до часа и более. Пожалуйста, будьте терпеливы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убедимся, что аудиофайл существует\n",
    "if not audio_path.is_file():\n",
    "    raise FileNotFoundError(\"Аудиофайл temp_audio.wav не найден. Убедитесь, что предыдущая ячейка выполнилась без ошибок.\")\n",
    "\n",
    "print(\"Запуск диаризации (определение спикеров)... Это может занять много времени.\")\n",
    "\n",
    "# Применяем pipeline диаризации к аудиофайлу.\n",
    "# Если вы знаете точное число говорящих, вы можете указать его (например, num_speakers=2), это может улучшить качество.\n",
    "diarization = diarization_pipeline(str(audio_path))\n",
    "\n",
    "print(\"Диаризация завершена. Начинаем транскрибацию каждого сегмента...\")\n",
    "\n",
    "# Список для хранения всех реплик в хронологическом порядке\n",
    "all_segments = []\n",
    "\n",
    "# Загружаем аудио целиком один раз, чтобы не делать это в цикле\n",
    "full_audio = whisper.load_audio(str(audio_path))\n",
    "\n",
    "# Обрабатываем результат диаризации\n",
    "for segment, track, speaker_label in diarization.itertracks(yield_label=True):\n",
    "    start_time = segment.start\n",
    "    end_time = segment.end\n",
    "    \n",
    "    # Вырезаем нужный сегмент из полного аудио\n",
    "    start_sample = int(start_time * 16000)\n",
    "    end_sample = int(end_time * 16000)\n",
    "    audio_segment = full_audio[start_sample:end_sample]\n",
    "    \n",
    "    # Преобразуем аудио-сегмент в тензор для модели\n",
    "    audio_tensor = torch.from_numpy(audio_segment).to(DEVICE)\n",
    "\n",
    "    # Опции для транскрибации\n",
    "    options = whisper.DecodingOptions(language=\"ru\", without_timestamps=True, fp16=torch.cuda.is_available())\n",
    "    \n",
    "    # Декодируем (транскрибируем) аудио сегмент\n",
    "    result = whisper.decode(whisper_model, audio_tensor.unsqueeze(0), options)\n",
    "    \n",
    "    if result.text.strip():\n",
    "        all_segments.append({\n",
    "            'start': start_time,\n",
    "            'speaker': speaker_label,\n",
    "            'text': result.text.strip()\n",
    "        })\n",
    "\n",
    "print(\"Транскрибация всех сегментов завершена.\")\n",
    "\n",
    "# Переменные для хранения результатов\n",
    "final_transcript_text = \"\"\n",
    "raw_text_for_summary = \"\"\n",
    "\n",
    "if all_segments:\n",
    "    # Сортируем все сегменты по времени начала, чтобы диалог был последовательным\n",
    "    all_segments.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    # Объединяем последовательные реплики от одного и того же спикера для красивого вывода\n",
    "    merged_transcript_lines = []\n",
    "    if all_segments:\n",
    "        current_speaker = all_segments[0]['speaker']\n",
    "        current_text = f\"**{current_speaker}:**\"\n",
    "\n",
    "        for seg in all_segments:\n",
    "            if seg['speaker'] == current_speaker:\n",
    "                current_text += \" \" + seg['text']\n",
    "            else:\n",
    "                merged_transcript_lines.append(current_text)\n",
    "                current_speaker = seg['speaker']\n",
    "                current_text = f\"**{current_speaker}:** {seg['text']}\"\n",
    "        merged_transcript_lines.append(current_text) # Добавляем последнюю реплику\n",
    "\n",
    "    final_transcript_text = \"\\n\\n\".join(merged_transcript_lines)\n",
    "    \n",
    "    # Создаем \"сырой\" текст для моделей NLP, просто соединяя все реплики\n",
    "    raw_text_for_summary = \" \".join([seg['text'] for seg in all_segments])\n",
    "else:\n",
    "    final_transcript_text = \"Не удалось распознать речь в аудиофайле.\"\n",
    "\n",
    "# Выводим итоговую транскрипцию\n",
    "display(Markdown(\"### Итоговая транскрипция:\"))\n",
    "display(Markdown(final_transcript_text))\n",
    "\n",
    "# Сохраняем \"сырой\" текст для следующих шагов\n",
    "print(\"\\nТранскрипт сохранен в переменную `raw_text_for_summary` для дальнейшего использования.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Общая Суммаризация текста\n",
    "\n",
    "Теперь, когда у нас есть полный текст, мы можем создать краткую выжимку. Так как текст очень длинный, мы будем использовать подход \"map-reduce\": разобьем текст на части, сделаем краткое содержание каждой части, а затем объединим эти содержания и сделаем итоговую выжимку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Загрузка модели для суммаризации...\")\n",
    "# Загружаем pipeline для суммаризации. Мы используем модель 'IlyaGusev/mbart_ru_sum_gazeta'.\n",
    "summarizer = pipeline(\"summarization\", model=\"IlyaGusev/mbart_ru_sum_gazeta\", device=DEVICE)\n",
    "print(\"Модель загружена.\")\n",
    "\n",
    "if 'raw_text_for_summary' in locals() and raw_text_for_summary:\n",
    "    print(\"Начинаем процесс суммаризации...\")\n",
    "    tokenizer = summarizer.tokenizer\n",
    "    tokens = tokenizer.encode(raw_text_for_summary)\n",
    "    chunk_size = 1024 # Максимальная длина для модели mBART\n",
    "    overlap = 100\n",
    "    \n",
    "    text_chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        text_chunks.append(tokenizer.decode(chunk_tokens, skip_special_tokens=True))\n",
    "        \n",
    "    print(f\"Текст разбит на {len(text_chunks)} частей для обработки.\")\n",
    "    \n",
    "    # Map-шаг\n",
    "    print(\"Создание выжимок для каждой части...\")\n",
    "    intermediate_summaries = summarizer(text_chunks, max_length=150, min_length=30, do_sample=False)\n",
    "    combined_summary_text = \" \".join([s['summary_text'] for s in intermediate_summaries])\n",
    "    \n",
    "    # Reduce-шаг\n",
    "    print(\"Создание итоговой выжимки...\")\n",
    "    final_summary_list = summarizer(combined_summary_text, max_length=300, min_length=60, do_sample=False)\n",
    "    general_summary = final_summary_list[0]['summary_text']\n",
    "    \n",
    "    display(Markdown(\"### Общее краткое содержание:\"))\n",
    "    display(Markdown(general_summary))\n",
    "else:\n",
    "    general_summary = \"Текст для суммаризации отсутствует.\"\n",
    "    display(Markdown(general_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: Ответы на конкретные вопросы\n",
    "\n",
    "Здесь вы можете задать вопрос к тексту интервью. Модель постарается найти на него ответ в транскрипции. Вы можете задавать вопросы несколько раз. Все они будут сохранены для итогового отчета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Загрузка модели для ответов на вопросы...\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepmipt/rubert-base-cased-squad-v1.1\", device=DEVICE)\n",
    "print(\"Модель загружена.\")\n",
    "\n",
    "# Список для хранения всех пар \"вопрос-ответ\"\n",
    "question_answers_list = []\n",
    "\n",
    "question_input = widgets.Textarea(description='Ваш вопрос:', layout=widgets.Layout(width='90%', height='80px'))\n",
    "ask_button = widgets.Button(description=\"Найти ответ\")\n",
    "answer_output = widgets.Output()\n",
    "\n",
    "def find_and_store_answer(b):\n",
    "    with answer_output:\n",
    "        answer_output.clear_output(wait=True)\n",
    "        question = question_input.value\n",
    "        if not question or 'raw_text_for_summary' not in locals() or not raw_text_for_summary:\n",
    "            print(\"Пожалуйста, введите вопрос и убедитесь, что транскрибация была выполнена.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Ищем ответ на вопрос: '{question}'...\")\n",
    "        result = qa_pipeline(question=question, context=raw_text_for_summary)\n",
    "        \n",
    "        # Сохраняем результат\n",
    "        qa_pair = {'question': question, 'answer': result['answer']}\n",
    "        question_answers_list.append(qa_pair)\n",
    "        \n",
    "        # Выводим результат\n",
    "        print(\"Найден следующий ответ:\")\n",
    "        display(Markdown(f\"> {result['answer']}\"))\n",
    "        print(f\"(Уверенность модели: {result['score']:.2%})\")\n",
    "        print(f\"Всего сохранено ответов: {len(question_answers_list)}\")\n",
    "\n",
    "ask_button.on_click(find_and_store_answer)\n",
    "display(question_input, ask_button, answer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5: Сохранение результатов в файл\n",
    "\n",
    "Последний шаг — сохранение всей проделанной работы в один текстовый файл. Файл `analysis_result.txt` будет создан в той же папке, где находится этот ноутбук."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем все ответы из предыдущей ячейки\n",
    "qa_summary_text = \"\"\n",
    "if 'question_answers_list' in locals() and question_answers_list:\n",
    "    qa_lines = [f\"Вопрос: {qa['question']}\\nОтвет: {qa['answer']}\" for qa in question_answers_list]\n",
    "    qa_summary_text = \"\\n\\n\".join(qa_lines)\n",
    "else:\n",
    "    qa_summary_text = \"Пользователь не задавал вопросов.\"\n",
    "\n",
    "# Формируем итоговый текст для сохранения, проверяя наличие переменных\n",
    "final_report_text = f\"\"\"\n",
    "========================================\n",
    "ИТОГОВЫЙ ОТЧЕТ ПО АНАЛИЗУ ВИДЕО\n",
    "========================================\n",
    "\n",
    "ОБЩЕЕ КРАТКОЕ СОДЕРЖАНИЕ\n",
    "------------------------\n",
    "{general_summary if 'general_summary' in locals() else 'Суммаризация не была выполнена.'}\n",
    "\n",
    "\n",
    "ОТВЕТЫ НА КОНКРЕТНЫЕ ВОПРОСЫ\n",
    "----------------------------\n",
    "{qa_summary_text}\n",
    "\n",
    "\n",
    "ПОЛНАЯ ТРАНСКРИПЦИЯ ДИАЛОГА\n",
    "----------------------------\n",
    "{final_transcript_text if 'final_transcript_text' in locals() else 'Транскрибация не была выполнена.'}\n",
    "\"\"\"\n",
    "\n",
    "output_filename = \"analysis_result.txt\"\n",
    "\n",
    "try:\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_report_text.strip())\n",
    "    print(f\"Отчет успешно сохранен в файл: {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при сохранении файла: {e}\")"
   ]
  }
 ]
}
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
